{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi agent Frozen Lake env\n",
    "An implmentation of a 2 agents version of gym's FrozenLake\n",
    "We can customize the map (with `S` the starting points of the two agents, `F` the floor cells, `H` the holes and `G` the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import pygame\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHHS\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"5x5\": [\n",
    "        \"SFFFH\",\n",
    "        \"HFFHF\",\n",
    "        \"FFFFH\",\n",
    "        \"HFHFF\",\n",
    "        \"SFHFG\"\n",
    "    ],\n",
    "    \"6x6\": [\n",
    "        \"SFFFHF\",\n",
    "        \"HFFHFF\",\n",
    "        \"FFHFHF\",\n",
    "        \"HFFFHH\",\n",
    "        \"FFHFHF\",\n",
    "        \"SFFFFG\"\n",
    "    ],\n",
    "    \"7x7\": [\n",
    "        \"SFFFFFH\",\n",
    "        \"HFFHFHF\",\n",
    "        \"FFHFFFH\",\n",
    "        \"HFFFHFF\",\n",
    "        \"FHFHFHF\",\n",
    "        \"HFFHFHS\",\n",
    "        \"GFFFFFH\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"SHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMap(num_agent, size, map_name=None, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a random FrozenLake map with a feasible path for each agent to the goal.\n",
    "\n",
    "    Args:\n",
    "        num_agent (int): Number of agents (starting positions).\n",
    "        size (int): Grid size (size x size).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the generated map.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    if map_name is not None:\n",
    "        return MAPS[map_name]\n",
    "    \n",
    "    # Define elements\n",
    "    FLOOR, HOLE, START, GOAL = 'F', 'H', 'S', 'G'\n",
    "    \n",
    "    # Step 1: Create an empty grid filled with 'F' (frozen floor)\n",
    "    grid = np.full((size, size), FLOOR)\n",
    "\n",
    "    # Step 2: Place the goal randomly\n",
    "    goal_pos = (random.randint(0, size - 1), random.randint(0, size - 1))\n",
    "    grid[goal_pos] = GOAL\n",
    "\n",
    "    # Step 3: Randomly place start positions, ensuring they are unique and not on the goal\n",
    "    start_positions = set()\n",
    "    while len(start_positions) < num_agent:\n",
    "        start_pos = (random.randint(0, size - 1), random.randint(0, size - 1))\n",
    "        if start_pos != goal_pos:\n",
    "            start_positions.add(start_pos)\n",
    "\n",
    "    for start_pos in start_positions:\n",
    "        grid[start_pos] = START\n",
    "\n",
    "    # Step 4: Randomly place holes\n",
    "    num_holes = max(1, int(0.2 * size * size))  # 20% of the grid is holes\n",
    "    hole_positions = set()\n",
    "    \n",
    "    while len(hole_positions) < num_holes:\n",
    "        hole_pos = (random.randint(0, size - 1), random.randint(0, size - 1))\n",
    "        if hole_pos != goal_pos and hole_pos not in start_positions:\n",
    "            hole_positions.add(hole_pos)\n",
    "    \n",
    "    for hole_pos in hole_positions:\n",
    "        grid[hole_pos] = HOLE\n",
    "\n",
    "    # Step 5: Check if all agents can reach the goal\n",
    "    def is_path_exists(start, goal, grid):\n",
    "        \"\"\"Check if there's a valid path using BFS.\"\"\"\n",
    "        rows, cols = grid.shape\n",
    "        queue = [start]\n",
    "        visited = set()\n",
    "\n",
    "        while queue:\n",
    "            x, y = queue.pop(0)\n",
    "            if (x, y) == goal:\n",
    "                return True\n",
    "            visited.add((x, y))\n",
    "\n",
    "            # Possible moves (Left, Down, Right, Up)\n",
    "            for dx, dy in [(-1, 0), (0, 1), (1, 0), (0, -1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited:\n",
    "                    if grid[nx, ny] != HOLE:\n",
    "                        queue.append((nx, ny))\n",
    "\n",
    "        return False\n",
    "\n",
    "    # Ensure every agent can reach the goal\n",
    "    for start in start_positions:\n",
    "        if not is_path_exists(start, goal_pos, grid):\n",
    "            return createMap(num_agent, size, seed + 1 if seed is not None else None)  # Retry with a different seed\n",
    "\n",
    "    # Convert to list of strings\n",
    "    return [\"\".join(row) for row in grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      "G . . . . . . G\n",
      "H . . . H A . .\n",
      ". A . . . . . H\n",
      ". . . . . . . H\n",
      ". . H . . . . H\n",
      ". . H . H . . .\n",
      ". . . A A . . .\n",
      "G . . H . H . G\n",
      "\n",
      "\n",
      "After a few random actions:\n",
      "G . . . . . . G\n",
      "H . . . A . . .\n",
      ". . . . . . . H\n",
      ". . . A . . . H\n",
      ". . H . . . . H\n",
      ". . A . A . . .\n",
      ". . . . . . . .\n",
      "G . . H . H . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "\n",
    "class MultiAgentFrozenLake4Goals(gym.Env):\n",
    "    def __init__(self, map_, max_steps=100, num_agents=2, collaboration_bonus=1.0, collision_penalty=0.3, seed=None):\n",
    "        # Load the map\n",
    "        self.desc = np.asarray(map_, dtype='c')\n",
    "        self.nrow, self.ncol = self.desc.shape\n",
    "        self.original_desc = self.desc.copy()  # Keep original for reference\n",
    "        self.num_agents = num_agents\n",
    "        self.max_steps = max_steps\n",
    "        self.collaboration_bonus = collaboration_bonus\n",
    "        self.collision_penalty = collision_penalty\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        if seed is not None:\n",
    "            self.np_random = np.random.RandomState(seed)\n",
    "            self.random_gen = random.Random(seed)\n",
    "        else:\n",
    "            self.np_random = np.random.RandomState()\n",
    "            self.random_gen = random.Random()\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Create a tuple of Discrete(4) for each agent\n",
    "        self.action_space = spaces.Tuple(tuple(spaces.Discrete(4) for _ in range(num_agents)))\n",
    "        \n",
    "        # State space: (agent1_row, agent1_col, agent2_row, agent2_col, ..., agentN_row, agentN_col)\n",
    "        self.observation_space = spaces.Tuple(\n",
    "            tuple(spaces.Discrete(self.nrow) for _ in range(num_agents)) +  # rows\n",
    "            tuple(spaces.Discrete(self.ncol) for _ in range(num_agents))    # columns\n",
    "        )\n",
    "        \n",
    "        # Define goal positions at each corner\n",
    "        self.goal_positions = [\n",
    "            (0, 0),                      # Top-left\n",
    "            (0, self.ncol-1),            # Top-right\n",
    "            (self.nrow-1, 0),            # Bottom-left\n",
    "            (self.nrow-1, self.ncol-1)   # Bottom-right\n",
    "        ]\n",
    "        \n",
    "        # Need to import colorsys for agent color generation\n",
    "        import colorsys\n",
    "        self.colorsys = colorsys\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        self.goals_reached_by_agent = None\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            self.np_random = np.random.RandomState(seed)\n",
    "            self.random_gen = random.Random(seed)\n",
    "        \n",
    "        self.desc = self.original_desc.copy()\n",
    "        \n",
    "        # Mark the corners as goals on the map\n",
    "        for goal_pos in self.goal_positions:\n",
    "            self.desc[goal_pos[0], goal_pos[1]] = b'G'\n",
    "        \n",
    "        # Find valid positions (not holes, not goals)\n",
    "        valid_positions = []\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                pos = (i, j)\n",
    "                if pos not in self.goal_positions and self.desc[i, j] != b'H':\n",
    "                    valid_positions.append(pos)\n",
    "        \n",
    "        # Initialize agent positions randomly\n",
    "        self.agent_positions = []\n",
    "        if len(valid_positions) >= self.num_agents:\n",
    "            # Shuffle valid positions\n",
    "            self.random_gen.shuffle(valid_positions)\n",
    "            # Pick first n positions for agents\n",
    "            self.agent_positions = [np.array(valid_positions[i]) for i in range(self.num_agents)]\n",
    "        else:\n",
    "            raise ValueError(f\"Not enough valid positions ({len(valid_positions)}) for {self.num_agents} agents.\")\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.agent_done = [False] * self.num_agents\n",
    "        self.reached_goal = [False] * self.num_agents\n",
    "        \n",
    "        # Track which goals each agent has reached\n",
    "        self.goals_reached_by_agent = [set() for _ in range(self.num_agents)]\n",
    "        \n",
    "        # Flatten the state: [agent1_row, agent1_col, agent2_row, agent2_col, ...]\n",
    "        self.state = tuple(pos for agent_pos in self.agent_positions for pos in agent_pos)\n",
    "        \n",
    "        # Initialize last actions for all agents (default to UP)\n",
    "        self.last_actions = [3] * self.num_agents\n",
    "        \n",
    "        return self.state, {}\n",
    "    \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Ensure actions is a list or tuple with the right length\n",
    "        if isinstance(actions, (int, np.integer)):  # Handle single agent case\n",
    "            actions = [actions]\n",
    "        elif not isinstance(actions, (list, tuple, np.ndarray)) or len(actions) != self.num_agents:\n",
    "            raise ValueError(f\"Expected actions to be a list/tuple of length {self.num_agents}, got {actions}\")\n",
    "        \n",
    "        # Store last actions for rendering\n",
    "        self.last_actions = list(actions)\n",
    "        \n",
    "        # Move all agents that aren't done\n",
    "        new_positions = []\n",
    "        for i, action in enumerate(actions):\n",
    "            if not self.agent_done[i]:\n",
    "                new_positions.append(self._move_agent(self.agent_positions[i], action))\n",
    "            else:\n",
    "                new_positions.append(self.agent_positions[i])\n",
    "        \n",
    "        # Check for collisions\n",
    "        collision_agents = self._check_collisions(new_positions)\n",
    "        \n",
    "        # Update agent positions\n",
    "        self.agent_positions = new_positions\n",
    "        \n",
    "        # Update state\n",
    "        self.state = tuple(pos for agent_pos in self.agent_positions for pos in agent_pos)\n",
    "        \n",
    "        # Initialize rewards, done flags, and info\n",
    "        rewards = [0] * self.num_agents\n",
    "        dones = [False] * self.num_agents\n",
    "        truncated = [False] * self.num_agents\n",
    "        info = {\"simultaneous_arrival\": False, \"collisions\": False}\n",
    "        \n",
    "        # Temporary flags to detect simultaneous goal arrival in this step\n",
    "        just_reached_goal = [False] * self.num_agents\n",
    "        \n",
    "        # Check status for each agent\n",
    "        for i in range(self.num_agents):\n",
    "            if not self.agent_done[i]:\n",
    "                agent_pos_tuple = tuple(self.agent_positions[i])\n",
    "                \n",
    "                # Check if agent fell in a hole\n",
    "                if self.desc[agent_pos_tuple[0], agent_pos_tuple[1]] == b'H':\n",
    "                    rewards[i] = 0\n",
    "                    self.agent_done[i] = True\n",
    "                    dones[i] = True\n",
    "                \n",
    "                # Check if agent reached a goal\n",
    "                elif agent_pos_tuple in self.goal_positions:\n",
    "                    # Check if this is a new goal the agent hasn't reached before\n",
    "                    if agent_pos_tuple not in self.goals_reached_by_agent[i]:\n",
    "                        # Add this goal to the set of goals reached by this agent\n",
    "                        self.goals_reached_by_agent[i].add(agent_pos_tuple)\n",
    "                        # Reward the agent 100 points for each unique goal reached\n",
    "                        rewards[i] = 100 * len(self.goals_reached_by_agent[i])\n",
    "                        just_reached_goal[i] = True\n",
    "                    \n",
    "                    # Mark the agent as done (can't move anymore) but episode continues\n",
    "                    self.agent_done[i] = True\n",
    "                    dones[i] = True\n",
    "                    self.reached_goal[i] = True\n",
    "            else:\n",
    "                # If agent was already done, keep the done flag set\n",
    "                dones[i] = True\n",
    "        \n",
    "        # Apply collision penalties if needed\n",
    "        if collision_agents:\n",
    "            # Apply collision penalty to agents involved in collisions\n",
    "            for agent_idx in collision_agents:\n",
    "                if not self.agent_done[agent_idx]:  # Only penalize active agents\n",
    "                    rewards[agent_idx] -= self.collision_penalty\n",
    "            \n",
    "            info[\"collisions\"] = True\n",
    "            info[\"collision_agents\"] = collision_agents\n",
    "        \n",
    "        # Check if multiple agents reached the goal in this step\n",
    "        simultaneous_arrivals = sum(just_reached_goal)\n",
    "        if simultaneous_arrivals >= 2:\n",
    "            # Calculate progressive bonus based on number of simultaneous arrivals\n",
    "            # More agents arriving together = higher bonus per agent\n",
    "            progressive_bonus = self.collaboration_bonus * (simultaneous_arrivals - 1)\n",
    "\n",
    "            # Apply the progressive collaboration bonus to all agents who arrived simultaneously\n",
    "            for i in range(self.num_agents):\n",
    "                if just_reached_goal[i]:\n",
    "                    rewards[i] += progressive_bonus\n",
    "\n",
    "            info[\"simultaneous_arrival\"] = True\n",
    "            info[\"num_simultaneous_arrivals\"] = simultaneous_arrivals\n",
    "            info[\"progressive_bonus\"] = progressive_bonus\n",
    "        \n",
    "        # Add information about how many unique goals each agent has reached\n",
    "        info[\"goals_reached\"] = [len(goals) for goals in self.goals_reached_by_agent]\n",
    "        \n",
    "        # Set truncated flag if max steps reached\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = [True] * self.num_agents\n",
    "        \n",
    "        return self.state, rewards, dones, truncated, info\n",
    "    \n",
    "    def _move_agent(self, position, action):\n",
    "        \"\"\"Move agent based on action\"\"\"\n",
    "        # Get new position\n",
    "        new_position = position.copy()\n",
    "        \n",
    "        # 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "        if action == 0:  # LEFT\n",
    "            new_position[1] = max(0, position[1] - 1)\n",
    "        elif action == 1:  # DOWN\n",
    "            new_position[0] = min(self.nrow - 1, position[0] + 1)\n",
    "        elif action == 2:  # RIGHT\n",
    "            new_position[1] = min(self.ncol - 1, position[1] + 1)\n",
    "        elif action == 3:  # UP\n",
    "            new_position[0] = max(0, position[0] - 1)\n",
    "        \n",
    "        return new_position\n",
    "    \n",
    "    def _check_collisions(self, positions):\n",
    "        \"\"\"\n",
    "        Check for collisions between agents and return a list of agents involved in collisions.\n",
    "        \"\"\"\n",
    "        collision_agents = set()\n",
    "        # Create a dictionary where keys are positions and values are lists of agent indices\n",
    "        position_to_agents = {}\n",
    "        \n",
    "        for i, pos in enumerate(positions):\n",
    "            if not self.agent_done[i]:  # Only check active agents\n",
    "                pos_tuple = tuple(pos)\n",
    "                if pos_tuple in position_to_agents:\n",
    "                    # Add all agents at this position to the collision set\n",
    "                    for agent_idx in position_to_agents[pos_tuple]:\n",
    "                        collision_agents.add(agent_idx)\n",
    "                    collision_agents.add(i)\n",
    "                    position_to_agents[pos_tuple].append(i)\n",
    "                else:\n",
    "                    position_to_agents[pos_tuple] = [i]\n",
    "                    \n",
    "        return list(collision_agents)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Simple text rendering of the environment\"\"\"\n",
    "        grid = np.full((self.nrow, self.ncol), \".\")\n",
    "        \n",
    "        # Mark goals\n",
    "        for goal in self.goal_positions:\n",
    "            grid[goal[0], goal[1]] = \"G\"\n",
    "        \n",
    "        # Mark holes\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                if self.desc[i, j] == b'H':\n",
    "                    grid[i, j] = \"H\"\n",
    "        \n",
    "        # Mark agents\n",
    "        for i, pos in enumerate(self.agent_positions):\n",
    "            # Check for collisions\n",
    "            pos_tuple = tuple(pos)\n",
    "            collision = False\n",
    "            for j, other_pos in enumerate(self.agent_positions):\n",
    "                if i != j and tuple(other_pos) == pos_tuple:\n",
    "                    collision = True\n",
    "                    break\n",
    "            \n",
    "            # Show \"C\" for collision or agent number\n",
    "            if collision:\n",
    "                grid[pos[0], pos[1]] = \"C\"\n",
    "            else:\n",
    "                grid[pos[0], pos[1]] = f\"A{i+1}\"\n",
    "        \n",
    "        # Print the grid\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print()\n",
    "\n",
    "    def render_pygame(self, screen_size=400):\n",
    "        \"\"\"Render the environment using pygame with the original gym images\"\"\"\n",
    "        # Initialize pygame if not already done\n",
    "        if not hasattr(self, 'pygame_initialized') or not self.pygame_initialized:\n",
    "            pygame.init()\n",
    "            self.pygame_initialized = True\n",
    "            self.screen = pygame.display.set_mode((screen_size, screen_size))\n",
    "            pygame.display.set_caption(f\"Multi-Agent Frozen Lake ({self.num_agents} agents)\")\n",
    "            self.cell_size = screen_size // max(self.nrow, self.ncol)\n",
    "            self.running = True  # Flag to control pygame loop\n",
    "            \n",
    "            # Load images from gym repository\n",
    "            img_dir = \"img/\"\n",
    "            self.images = {\n",
    "                'F': pygame.image.load(img_dir + \"ice.png\"),\n",
    "                'H': pygame.image.load(img_dir + \"hole.png\"),\n",
    "                'G': pygame.image.load(img_dir + \"ice.png\"),  # Use ice as background for goal\n",
    "                'S': pygame.image.load(img_dir + \"stool.png\")\n",
    "            }\n",
    "            \n",
    "            # Load goal sprite separately to overlay on ice\n",
    "            self.goal_sprite = pygame.image.load(img_dir + \"goal.png\")\n",
    "            self.goal_sprite = pygame.transform.scale(self.goal_sprite, (self.cell_size, self.cell_size))\n",
    "            \n",
    "            # Load agent images for different directions\n",
    "            self.agent_images = {\n",
    "                'up': pygame.image.load(img_dir + \"elf_up.png\"),\n",
    "                'down': pygame.image.load(img_dir + \"elf_down.png\"),\n",
    "                'left': pygame.image.load(img_dir + \"elf_left.png\"),\n",
    "                'right': pygame.image.load(img_dir + \"elf_right.png\")\n",
    "            }\n",
    "            \n",
    "            # Resize images to fit the cell size\n",
    "            for key in self.images:\n",
    "                self.images[key] = pygame.transform.scale(self.images[key], (self.cell_size, self.cell_size))\n",
    "            \n",
    "            for key in self.agent_images:\n",
    "                self.agent_images[key] = pygame.transform.scale(self.agent_images[key], (self.cell_size, self.cell_size))\n",
    "            \n",
    "            # Create red-tinted version for collision\n",
    "            self.collision_images = {}\n",
    "            \n",
    "            for key, img in self.agent_images.items():\n",
    "                # Create red-tinted version for collision\n",
    "                self.collision_images[key] = img.copy()\n",
    "                red_surface = pygame.Surface(img.get_size(), pygame.SRCALPHA)\n",
    "                red_surface.fill((255, 0, 0, 100))  # Red tint\n",
    "                self.collision_images[key].blit(red_surface, (0, 0), special_flags=pygame.BLEND_RGBA_ADD)\n",
    "            \n",
    "            # Define agent indicator colors - generate a unique color for each agent\n",
    "            self.agent_colors = []\n",
    "            for i in range(self.num_agents):\n",
    "                # Generate a unique color based on the agent index\n",
    "                hue = i / max(1, self.num_agents)\n",
    "                rgb = self.colorsys.hsv_to_rgb(hue, 1.0, 1.0)\n",
    "                self.agent_colors.append((int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)))\n",
    "        \n",
    "        # Don't render if pygame has been closed\n",
    "        if not hasattr(self, 'running') or not self.running:\n",
    "            return\n",
    "        \n",
    "        # Clear screen\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                # Determine tile type\n",
    "                if (i, j) in self.goal_positions:\n",
    "                    tile_char = 'G'\n",
    "                elif self.desc[i, j] == b'H':\n",
    "                    tile_char = 'H'\n",
    "                else:\n",
    "                    tile_char = 'F'\n",
    "                \n",
    "                rect = pygame.Rect(j * self.cell_size, i * self.cell_size,\n",
    "                                self.cell_size, self.cell_size)\n",
    "                # Draw tile\n",
    "                self.screen.blit(self.images[tile_char], rect)\n",
    "                \n",
    "                # Overlay goal sprite on ice if this is the goal position\n",
    "                if tile_char == 'G':\n",
    "                    self.screen.blit(self.goal_sprite, rect)\n",
    "        \n",
    "        # Draw grid lines\n",
    "        grid_color = (50, 50, 50)  # Dark gray\n",
    "        for i in range(self.nrow + 1):\n",
    "            pygame.draw.line(self.screen, grid_color, \n",
    "                            (0, i * self.cell_size), \n",
    "                            (self.ncol * self.cell_size, i * self.cell_size), 2)\n",
    "        for j in range(self.ncol + 1):\n",
    "            pygame.draw.line(self.screen, grid_color, \n",
    "                            (j * self.cell_size, 0), \n",
    "                            (j * self.cell_size, self.nrow * self.cell_size), 2)\n",
    "        \n",
    "        # Map actions to directions for agent images\n",
    "        direction_map = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "        \n",
    "        # Check for collisions (where 2 or more agents occupy the same cell)\n",
    "        # Create a dictionary to track cell occupancy\n",
    "        cell_occupancy = {}\n",
    "        for i, agent_pos in enumerate(self.agent_positions):\n",
    "            pos_tuple = tuple(agent_pos)\n",
    "            if pos_tuple in cell_occupancy:\n",
    "                cell_occupancy[pos_tuple].append(i)\n",
    "            else:\n",
    "                cell_occupancy[pos_tuple] = [i]\n",
    "        \n",
    "        # Draw agents\n",
    "        for i, agent_pos in enumerate(self.agent_positions):\n",
    "            pos_tuple = tuple(agent_pos)\n",
    "            agent_rect = pygame.Rect(agent_pos[1] * self.cell_size, \n",
    "                                   agent_pos[0] * self.cell_size,\n",
    "                                   self.cell_size, self.cell_size)\n",
    "            \n",
    "            # Make sure we have a valid action index\n",
    "            action_idx = min(self.last_actions[i], 3) if i < len(self.last_actions) else 3\n",
    "            \n",
    "            # Check if this agent is colliding with others\n",
    "            is_collision = len(cell_occupancy[pos_tuple]) > 1\n",
    "            \n",
    "            if is_collision:\n",
    "                # Use red-tinted image for collisions\n",
    "                collision_img = self.collision_images[direction_map[action_idx]]\n",
    "                self.screen.blit(collision_img, agent_rect)\n",
    "                \n",
    "                # Draw indicators for all colliding agents\n",
    "                colliding_agents = cell_occupancy[pos_tuple]\n",
    "                for idx, colliding_agent_idx in enumerate(colliding_agents):\n",
    "                    # Position indicators at different corners\n",
    "                    indicator_size = max(4, int(self.cell_size / 8))\n",
    "                    \n",
    "                    # Calculate position based on index (up to 4 agents per cell)\n",
    "                    corner_idx = idx % 4\n",
    "                    if corner_idx == 0:  # Top-left\n",
    "                        pos = (agent_rect.left + indicator_size, agent_rect.top + indicator_size)\n",
    "                    elif corner_idx == 1:  # Top-right\n",
    "                        pos = (agent_rect.right - indicator_size, agent_rect.top + indicator_size)\n",
    "                    elif corner_idx == 2:  # Bottom-left\n",
    "                        pos = (agent_rect.left + indicator_size, agent_rect.bottom - indicator_size)\n",
    "                    else:  # Bottom-right\n",
    "                        pos = (agent_rect.right - indicator_size, agent_rect.bottom - indicator_size)\n",
    "                    \n",
    "                    # Draw the indicator\n",
    "                    if colliding_agent_idx < len(self.agent_colors):\n",
    "                        pygame.draw.circle(self.screen, self.agent_colors[colliding_agent_idx], pos, indicator_size)\n",
    "            else:\n",
    "                # Draw agent normally\n",
    "                agent_img = self.agent_images[direction_map[action_idx]]\n",
    "                self.screen.blit(agent_img, agent_rect)\n",
    "                \n",
    "                # Draw a small indicator for agent identification\n",
    "                indicator_size = max(4, int(self.cell_size / 8))\n",
    "                pygame.draw.circle(self.screen, self.agent_colors[i], \n",
    "                                (agent_rect.left + indicator_size, agent_rect.top + indicator_size), \n",
    "                                indicator_size)\n",
    "        \n",
    "        # Update display\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # Process events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.running = False\n",
    "                pygame.quit()\n",
    "                return\n",
    "            \n",
    "    def close(self):\n",
    "        \"\"\"Properly close the pygame window\"\"\"\n",
    "        if hasattr(self, 'pygame_initialized') and self.pygame_initialized:\n",
    "            self.running = False\n",
    "            pygame.quit()\n",
    "            self.pygame_initialized = False\n",
    "\n",
    "def generate_map(size=8, hole_prob=0.1, seed=None):\n",
    "    \"\"\"Generate a random map with given dimensions and hole probability.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Create an empty map\n",
    "    map_grid = [['F' for _ in range(size)] for _ in range(size)]\n",
    "    \n",
    "    # Place holes randomly\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            # Skip corners (reserve for goals)\n",
    "            if (i == 0 and j == 0) or (i == 0 and j == size-1) or \\\n",
    "               (i == size-1 and j == 0) or (i == size-1 and j == size-1):\n",
    "                continue\n",
    "            \n",
    "            if np.random.random() < hole_prob:\n",
    "                map_grid[i][j] = 'H'\n",
    "    \n",
    "    # Convert to the expected format\n",
    "    return map_grid\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate a random map\n",
    "    map_size = 8\n",
    "    map_grid = generate_map(size=map_size, hole_prob=0.15, seed=42)\n",
    "    \n",
    "    # Create the environment\n",
    "    env = MultiAgentFrozenLake4Goals(\n",
    "        map_=map_grid,\n",
    "        max_steps=100,\n",
    "        num_agents=4,\n",
    "        collaboration_bonus=1.0,\n",
    "        collision_penalty=0.3,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    \n",
    "    # Render the initial state\n",
    "    print(\"Initial state:\")\n",
    "    env.render()\n",
    "    \n",
    "    # Try running a few random actions\n",
    "    print(\"\\nAfter a few random actions:\")\n",
    "    for _ in range(5):\n",
    "        actions = [np.random.randint(0, 4) for _ in range(env.num_agents)]\n",
    "        state, rewards, dones, truncated, info = env.step(actions)\n",
    "        \n",
    "    env.render()\n",
    "    \n",
    "    # Close the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, action_size=4):\n",
    "        self.action_size = action_size\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action1 = np.random.randint(0, self.action_size)\n",
    "        action2 = np.random.randint(0, self.action_size)\n",
    "        return (action1, action2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Q Learning algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralQLearning:\n",
    "    def __init__(self, state_size, action_size, num_agents=2, learning_rate=0.1, discount_factor=0.99, \n",
    "                 exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01):\n",
    "        # Add num_agents as a parameter\n",
    "        self.num_agents = num_agents\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.epsilon_decay = exploration_decay\n",
    "        self.epsilon_min = min_exploration_rate\n",
    "        \n",
    "        # Create a joint Q-table that can handle any number of agents\n",
    "        # We'll use a dictionary for states and a numpy array for joint actions\n",
    "        self.q_table = defaultdict(lambda: np.zeros([action_size] * num_agents))\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state_tuple = tuple(state)  # Convert state to tuple for dictionary key\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Random actions for all agents\n",
    "            actions = tuple(np.random.randint(0, self.action_size) for _ in range(self.num_agents))\n",
    "        else:\n",
    "            # Greedy action selection\n",
    "            # Use np.unravel_index with the shape based on num_agents\n",
    "            actions = np.unravel_index(\n",
    "                np.argmax(self.q_table[state_tuple]), \n",
    "                [self.action_size] * self.num_agents\n",
    "            )   \n",
    "        \n",
    "        return actions\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_tuple = tuple(state)\n",
    "        next_state_tuple = tuple(next_state)\n",
    "        \n",
    "        # Index into the Q-table using the full joint action\n",
    "        # We need to convert action tuple to a tuple of ints for proper indexing\n",
    "        action_tuple = tuple(int(a) for a in action)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state_tuple][action_tuple]\n",
    "        \n",
    "        # Next Q-value (maximum over all joint actions)\n",
    "        next_q = np.max(self.q_table[next_state_tuple]) if not done else 0\n",
    "        \n",
    "        # Q-value update\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * next_q - current_q)\n",
    "        self.q_table[state_tuple][action_tuple] = new_q\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_policies(trained_agent, num_episodes=100):\n",
    "    env = MultiAgentFrozenLake(map_name=\"4x4\", max_steps=100)\n",
    "    random_agent = RandomPolicy()\n",
    "    \n",
    "    # Evaluate trained policy\n",
    "    trained_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            state_tuple = tuple(state)\n",
    "            joint_actions = np.unravel_index(np.argmax(trained_agent.q_table[state_tuple]), \n",
    "                                        (trained_agent.action_size, trained_agent.action_size))\n",
    "            action = joint_actions\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        trained_rewards.append(episode_reward)\n",
    "    \n",
    "    # Evaluate random policy\n",
    "    random_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            action = random_agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        random_rewards.append(episode_reward)\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"Trained Policy - Average Reward: {np.mean(trained_rewards):.4f}, Success Rate: {(np.array(trained_rewards) > 0).mean():.2%}\")\n",
    "    print(f\"Random Policy  - Average Reward: {np.mean(random_rewards):.4f}, Success Rate: {(np.array(random_rewards) > 0).mean():.2%}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Trained Policy', 'Random Policy'], \n",
    "            [np.mean(trained_rewards), np.mean(random_rewards)],\n",
    "            yerr=[np.std(trained_rewards), np.std(random_rewards)])\n",
    "    plt.title('Policy Comparison')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(map_, agent, num_episodes=2, max_steps=20, use_pygame=True, num_agents=2):\n",
    "    \"\"\"Visualize the learned policy\"\"\"\n",
    "    env = MultiAgentFrozenLake(map_=map_, num_agents=num_agents)\n",
    "    \n",
    "    # Action names for better visualization\n",
    "    action_names = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "    \n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            print(f\"\\n=== Test Episode {i+1} ===\")\n",
    "            if use_pygame:\n",
    "                env.render_pygame()\n",
    "            else:\n",
    "                print(\"Initial state:\")\n",
    "                env.render()\n",
    "            \n",
    "            while not done and not truncated and steps < max_steps:\n",
    "                # Use trained policy (no exploration)\n",
    "                state_tuple = tuple(state)\n",
    "                \n",
    "                # Get actions based on agent's Q-table\n",
    "                # This assumes agent.q_table is structured to handle num_agents\n",
    "                joint_actions = np.unravel_index(\n",
    "                    np.argmax(agent.q_table[state_tuple]),\n",
    "                    tuple([agent.action_size] * num_agents)\n",
    "                )\n",
    "                action = joint_actions\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Check for overlaps - this needs to be generalized for multiple agents\n",
    "                overlaps = []\n",
    "                for i in range(num_agents):\n",
    "                    for j in range(i+1, num_agents):\n",
    "                        # Compare positions of each pair of agents\n",
    "                        agent_i_pos = (next_state[i*2], next_state[i*2 + 1])\n",
    "                        agent_j_pos = (next_state[j*2], next_state[j*2 + 1])\n",
    "                        if agent_i_pos == agent_j_pos:\n",
    "                            overlaps.append((i, j))\n",
    "                \n",
    "                # Update state and reward\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Render with action information\n",
    "                print(f\"Step {steps}:\")\n",
    "                for agent_idx in range(num_agents):\n",
    "                    print(f\"Agent {agent_idx+1}: {action_names[action[agent_idx]]}\")\n",
    "                print(f\"Reward: {reward}\")\n",
    "                \n",
    "                if overlaps:\n",
    "                    print(\"Overlaps detected between agents:\", overlaps)\n",
    "                \n",
    "                if use_pygame:\n",
    "                    env.render_pygame()\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    env.render()\n",
    "                    time.sleep(0.5)\n",
    "            \n",
    "            print(f\"Episode finished after {steps} steps with total reward: {total_reward}\")\n",
    "            if done and total_reward > 0:\n",
    "                print(\"Success! At least one agent reached the goal.\")\n",
    "            elif done and total_reward <= 0:\n",
    "                print(\"Failed. Agents fell into holes or couldn't reach the goal.\")\n",
    "            else:\n",
    "                print(\"Truncated. Maximum steps reached.\")\n",
    "            \n",
    "            # Short pause between episodes\n",
    "            time.sleep(1)\n",
    "        \n",
    "    # Only close environment once after all episodes\n",
    "    finally:\n",
    "        # Make sure we close properly even if there's an exception\n",
    "        env.close()\n",
    "        if pygame.get_init():  # Check if pygame is still initialized\n",
    "            pygame.quit()  # Quit pygame completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(map_, num_agent, learning_rate, discount_factor, explo_rate, explo_decay, min_explo_rate, num_episodes=10000, silent=True):  # Increased episodes\n",
    "    # Create environment\n",
    "    env = MultiAgentFrozenLake4Goals(map_=map_, max_steps=100, num_agents=num_agent)\n",
    "    \n",
    "    # Create CentralQLearning agent with adjusted parameters\n",
    "    state_size = env.nrow * env.ncol * env.nrow * env.ncol\n",
    "    action_size = 4\n",
    "    agent = CentralQLearning(state_size=state_size, action_size=action_size,num_agents=num_agent, \n",
    "                           learning_rate=learning_rate,\n",
    "                           discount_factor=discount_factor,\n",
    "                           exploration_rate=explo_rate,\n",
    "                           exploration_decay=explo_decay,\n",
    "                           min_exploration_rate=min_explo_rate)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    episode_rewards = []\n",
    "    success_rate = []\n",
    "    success_window = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while not done and not truncated:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Record episode success/failure\n",
    "        success = total_reward > 0\n",
    "        success_window.append(success)\n",
    "        if len(success_window) > 200:\n",
    "            success_window.pop(0)\n",
    "        \n",
    "        # Calculate success rate over last 100 episodes\n",
    "        current_success_rate = sum(success_window) / len(success_window)\n",
    "        success_rate.append(current_success_rate)\n",
    "        \n",
    "        # Record total reward\n",
    "        mean_reward = total_reward / step if step > 0 else 0\n",
    "        episode_rewards.append(mean_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if not silent and episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {mean_reward}, Success Rate: {current_success_rate:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    window_size = 200\n",
    "    mean_rewards_smooth = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(mean_rewards_smooth)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(success_rate)\n",
    "    plt.title('Success Rate (100-episode moving average)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agents...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"float\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m seed            \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m map_ \u001b[38;5;241m=\u001b[39m createMap(num_agent, map_size, map_name, seed)\n\u001b[1;32m---> 18\u001b[0m trained_agent   \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplo_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplo_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_explo_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_ep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compare policies\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"Comparing trained policy with random policy...\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# compare_policies(trained_agent)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Visualize the learned policy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(map_, num_agent, learning_rate, discount_factor, explo_rate, explo_decay, min_explo_rate, num_episodes, silent)\u001b[0m\n\u001b[0;32m     32\u001b[0m next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Update Q-table\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Update state and total reward\u001b[39;00m\n\u001b[0;32m     36\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m, in \u001b[0;36mCentralQLearning.update\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     46\u001b[0m next_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[next_state_tuple]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Q-value update\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m new_q \u001b[38;5;241m=\u001b[39m current_q \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m (\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_q\u001b[49m \u001b[38;5;241m-\u001b[39m current_q)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state_tuple][action_tuple] \u001b[38;5;241m=\u001b[39m new_q\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Decay exploration rate\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"float\") to list"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the agents...\")\n",
    "    \n",
    "    num_agent = 6\n",
    "    \n",
    "    n_ep            = 10000#0\n",
    "    learning_rate   = 0.1\n",
    "    discount_factor = 0.1\n",
    "    explo_rate      = 1.0\n",
    "    explo_decay     = 0.999#95\n",
    "    min_explo_rate  = 0.05\n",
    "    map_name        = '8x8'\n",
    "    map_size        = 5\n",
    "    seed            = 0\n",
    "    \n",
    "    map_ = createMap(num_agent, map_size, map_name, seed)\n",
    "    \n",
    "    trained_agent   = run_simulation(map_, num_agent, learning_rate, discount_factor, explo_rate, explo_decay, min_explo_rate, num_episodes=n_ep)\n",
    "    print(\"Training complete!\")\n",
    "    # Compare policies\n",
    "    # print(\"Comparing trained policy with random policy...\")\n",
    "    # compare_policies(trained_agent)\n",
    "    \n",
    "    # Visualize the learned policy\n",
    "    print(\"Visualizing the learned policy...\")\n",
    "    visualize_policy(map_, trained_agent, num_episodes=3, num_agents=num_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
