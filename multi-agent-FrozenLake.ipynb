{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi agent Frozen Lake env\n",
    "An implmentation of a 2 agents version of gym's FrozenLake\n",
    "We can customize the map (with `S` the starting points of the two agents, `F` the floor cells, `H` the holes and `G` the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import pygame\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHSH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"SHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentFrozenLake(gym.Env):\n",
    "    def __init__(self, map_name=\"4x4\", max_steps=100):\n",
    "        # Load the map\n",
    "        self.desc = np.asarray(MAPS[map_name], dtype='c')\n",
    "        self.nrow, self.ncol = self.desc.shape\n",
    "        self.original_desc = self.desc.copy()  # Keep original for reference\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(4), spaces.Discrete(4)))  # Actions for both agents\n",
    "        \n",
    "        # State space: (agent1_row, agent1_col, agent2_row, agent2_col)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.nrow), \n",
    "            spaces.Discrete(self.ncol),\n",
    "            spaces.Discrete(self.nrow), \n",
    "            spaces.Discrete(self.ncol)\n",
    "        ))\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.np_random = np.random.RandomState()\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            self.np_random = np.random.RandomState(seed)\n",
    "        \n",
    "        self.desc = self.original_desc.copy()\n",
    "        start_positions = np.argwhere(self.desc == b'S')\n",
    "        goal_position = np.argwhere(self.desc == b'G')[0]\n",
    "        \n",
    "        if len(start_positions) > 1:\n",
    "            self.agent1_pos = start_positions[0]\n",
    "            self.agent2_pos = start_positions[1]\n",
    "        else:\n",
    "            self.agent1_pos = start_positions[0]\n",
    "            self.agent2_pos = start_positions[0].copy()\n",
    "        \n",
    "        self.goal_pos = goal_position\n",
    "        self.steps = 0\n",
    "        self.agents_goal_steps = [None, None]\n",
    "        \n",
    "        self.state = (self.agent1_pos[0], self.agent1_pos[1], self.agent2_pos[0], self.agent2_pos[1])\n",
    "        return self.state, {}\n",
    "    \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        action1, action2 = actions\n",
    "        \n",
    "        if self.steps == 1:\n",
    "            start_positions = np.argwhere(self.desc == b'S')\n",
    "            for pos in start_positions:\n",
    "                self.desc[pos[0], pos[1]] = b'F'\n",
    "        \n",
    "        self.agent1_pos = self._move_agent(self.agent1_pos, action1)\n",
    "        self.agent2_pos = self._move_agent(self.agent2_pos, action2)\n",
    "        \n",
    "        done1 = np.array_equal(self.agent1_pos, self.goal_pos)\n",
    "        done2 = np.array_equal(self.agent2_pos, self.goal_pos)\n",
    "        \n",
    "        if done1 and self.agents_goal_steps[0] is None:\n",
    "            self.agents_goal_steps[0] = self.steps\n",
    "        if done2 and self.agents_goal_steps[1] is None:\n",
    "            self.agents_goal_steps[1] = self.steps\n",
    "        \n",
    "        self.state = (self.agent1_pos[0], self.agent1_pos[1], self.agent2_pos[0], self.agent2_pos[1])\n",
    "        done = (done1 and done2) or self.steps >= self.max_steps\n",
    "        \n",
    "        agent1_fallen = self.desc[self.agent1_pos[0], self.agent1_pos[1]] == b'H'\n",
    "        agent2_fallen = self.desc[self.agent2_pos[0], self.agent2_pos[1]] == b'H'\n",
    "        \n",
    "        if agent1_fallen or agent2_fallen:\n",
    "            reward = -5.0  # Immediate failure if any agent falls\n",
    "            done = True\n",
    "        elif done1 and done2:\n",
    "            if self.agents_goal_steps[0] == self.agents_goal_steps[1]:\n",
    "                reward = 1.0  # Full reward if they arrive together\n",
    "            else:\n",
    "                reward = 0.5  # Partial reward if they arrive at different times\n",
    "            done = True\n",
    "        elif done1 or done2:\n",
    "            reward = -0.2  # Penalize reaching the goal alone\n",
    "        else:\n",
    "            reward = -0.01  # Small penalty to encourage movement\n",
    "\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        return self.state, reward, done, truncated, {}\n",
    "    \n",
    "    def _move_agent(self, position, action):\n",
    "        # Get new position\n",
    "        new_position = position.copy()\n",
    "        \n",
    "        # 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "        if action == 0:  # LEFT\n",
    "            new_position[1] = max(0, position[1] - 1)\n",
    "        elif action == 1:  # DOWN\n",
    "            new_position[0] = min(self.nrow - 1, position[0] + 1)\n",
    "        elif action == 2:  # RIGHT\n",
    "            new_position[1] = min(self.ncol - 1, position[1] + 1)\n",
    "        elif action == 3:  # UP\n",
    "            new_position[0] = max(0, position[0] - 1)\n",
    "        \n",
    "        # Check if new position is a hole or valid\n",
    "        if self.desc[new_position[0], new_position[1]] != b'H':\n",
    "            return new_position\n",
    "        else:\n",
    "            # If it's a hole, agent falls in\n",
    "            return new_position\n",
    "\n",
    "    def render_pygame(self, screen_size=400):\n",
    "        '''Render the environment using pygame with color change on collision'''\n",
    "        # Initialize pygame if not already done\n",
    "        if not hasattr(self, 'pygame_initialized'):\n",
    "            pygame.init()\n",
    "            self.pygame_initialized = True\n",
    "            self.screen = pygame.display.set_mode((screen_size, screen_size))\n",
    "            pygame.display.set_caption(\"Multi-Agent Frozen Lake\")\n",
    "            self.cell_size = screen_size // max(self.nrow, self.ncol)\n",
    "            self.font = pygame.font.Font(None, self.cell_size // 2)\n",
    "        \n",
    "        # Clear screen\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                rect = pygame.Rect(j * self.cell_size, i * self.cell_size, \n",
    "                                self.cell_size, self.cell_size)\n",
    "                \n",
    "                # Color based on cell type\n",
    "                if self.desc[i, j] == b'F':\n",
    "                    color = (173, 216, 230)  # Light blue for frozen\n",
    "                elif self.desc[i, j] == b'H':\n",
    "                    color = (139, 69, 19)  # Brown for hole\n",
    "                elif self.desc[i, j] == b'G':\n",
    "                    color = (0, 255, 0)  # Green for goal\n",
    "                elif self.desc[i, j] == b'S':\n",
    "                    color = (255, 255, 255)  # White for start\n",
    "                else:\n",
    "                    color = (0, 0, 0)  # Black for unknown\n",
    "                \n",
    "                pygame.draw.rect(self.screen, color, rect)\n",
    "                pygame.draw.rect(self.screen, (0, 0, 0), rect, 1)  # Grid lines\n",
    "                \n",
    "                # Draw cell letter\n",
    "                text = self.font.render(self.desc[i, j].decode('utf-8'), True, (0, 0, 0))\n",
    "                text_rect = text.get_rect(center=rect.center)\n",
    "                self.screen.blit(text, text_rect)\n",
    "        \n",
    "        # Check for collision (overlap)\n",
    "        collision = np.array_equal(self.agent1_pos, self.agent2_pos)\n",
    "        \n",
    "        # Determine agent colors\n",
    "        if collision:\n",
    "            agent1_color = agent2_color = (128, 0, 128)  # Purple when overlapping\n",
    "        else:\n",
    "            agent1_color = (255, 0, 0)  # Red for agent 1\n",
    "            agent2_color = (0, 0, 255)  # Blue for agent 2\n",
    "\n",
    "        # Draw agents\n",
    "        for agent_pos, agent_color in [\n",
    "            (self.agent1_pos, agent1_color),\n",
    "            (self.agent2_pos, agent2_color)\n",
    "        ]:\n",
    "            rect = pygame.Rect(agent_pos[1] * self.cell_size, agent_pos[0] * self.cell_size, \n",
    "                            self.cell_size, self.cell_size)\n",
    "            pygame.draw.circle(self.screen, agent_color, rect.center, self.cell_size // 3)\n",
    "        \n",
    "        # Update display\n",
    "        pygame.display.flip()\n",
    "            # Process events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, action_size=4):\n",
    "        self.action_size = action_size\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action1 = np.random.randint(0, self.action_size)\n",
    "        action2 = np.random.randint(0, self.action_size)\n",
    "        return (action1, action2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Q Learning algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralQLearning:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, \n",
    "                 exploration_decay=0.995, min_exploration_rate=0.01):\n",
    "        # Joint Q-table: state -> (action1, action2) -> Q-value\n",
    "        self.q_table = defaultdict(lambda: np.zeros((action_size, action_size)))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.epsilon_decay = exploration_decay\n",
    "        self.epsilon_min = min_exploration_rate\n",
    "        self.action_size = action_size\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tuple = tuple(state)  # Convert state to tuple for dictionary key\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Random actions for both agents\n",
    "            action1 = np.random.randint(0, self.action_size)\n",
    "            action2 = np.random.randint(0, self.action_size)\n",
    "        else:\n",
    "            # Greedy action selection\n",
    "            joint_actions = np.unravel_index(np.argmax(self.q_table[state_tuple]), \n",
    "                                            (self.action_size, self.action_size))\n",
    "            action1, action2 = joint_actions\n",
    "        \n",
    "        return (action1, action2)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_tuple = tuple(state)\n",
    "        next_state_tuple = tuple(next_state)\n",
    "        action1, action2 = action\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state_tuple][action1, action2]\n",
    "        \n",
    "        # Next Q-value (maximum over all joint actions)\n",
    "        next_q = np.max(self.q_table[next_state_tuple]) if not done else 0\n",
    "        \n",
    "        # Q-value update\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * next_q - current_q)\n",
    "        self.q_table[state_tuple][action1, action2] = new_q\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_policies(trained_agent, num_episodes=100):\n",
    "    env = MultiAgentFrozenLake(map_name=\"4x4\", max_steps=100)\n",
    "    random_agent = RandomPolicy()\n",
    "    \n",
    "    # Evaluate trained policy\n",
    "    trained_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            state_tuple = tuple(state)\n",
    "            joint_actions = np.unravel_index(np.argmax(trained_agent.q_table[state_tuple]), \n",
    "                                        (trained_agent.action_size, trained_agent.action_size))\n",
    "            action = joint_actions\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        trained_rewards.append(episode_reward)\n",
    "    \n",
    "    # Evaluate random policy\n",
    "    random_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            action = random_agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        random_rewards.append(episode_reward)\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"Trained Policy - Average Reward: {np.mean(trained_rewards):.4f}, Success Rate: {(np.array(trained_rewards) > 0).mean():.2%}\")\n",
    "    print(f\"Random Policy  - Average Reward: {np.mean(random_rewards):.4f}, Success Rate: {(np.array(random_rewards) > 0).mean():.2%}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Trained Policy', 'Random Policy'], \n",
    "            [np.mean(trained_rewards), np.mean(random_rewards)],\n",
    "            yerr=[np.std(trained_rewards), np.std(random_rewards)])\n",
    "    plt.title('Policy Comparison')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(agent, num_episodes=2, max_steps=20, use_pygame=True):\n",
    "    \"\"\"Visualize the learned policy\"\"\"\n",
    "    env = MultiAgentFrozenLake(map_name=\"4x4\")\n",
    "    \n",
    "    # Action names for better visualization\n",
    "    action_names = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        print(f\"\\n=== Test Episode {i+1} ===\")\n",
    "        \n",
    "        if use_pygame:\n",
    "            env.render_pygame()\n",
    "        else:\n",
    "            print(\"Initial state:\")\n",
    "            env.render()\n",
    "        \n",
    "        while not done and not truncated and steps < max_steps:\n",
    "            # Use trained policy (no exploration)\n",
    "            state_tuple = tuple(state)\n",
    "            joint_actions = np.unravel_index(np.argmax(agent.q_table[state_tuple]), \n",
    "                                           (agent.action_size, agent.action_size))\n",
    "            action = joint_actions\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            x1, y1, x2, y2 = next_state\n",
    "            overlap = (x1, y1) == (x2, y2)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Render with action information\n",
    "            print(f\"Step {steps}:\")\n",
    "            print(f\"Agent 1: {action_names[action[0]]}, Agent 2: {action_names[action[1]]}\")\n",
    "            print(f\"Reward: {reward}\")    \n",
    "            if overlap:\n",
    "                print(\"/!\\ Collision detected! Both agents are on the same tile. /!\\ \")\n",
    "            \n",
    "            if use_pygame:\n",
    "                env.render_pygame()\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                env.render()\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "        print(f\"Episode finished after {steps} steps with total reward: {total_reward}\")\n",
    "        if done and total_reward > 0:\n",
    "            print(\"Success! At least one agent reached the goal.\")\n",
    "        elif done and total_reward <= 0:\n",
    "            print(\"Failed. Agents fell into holes or couldn't reach the goal.\")\n",
    "        else:\n",
    "            print(\"Truncated. Maximum steps reached.\")\n",
    "        \n",
    "        # Short pause between episodes\n",
    "        time.sleep(1)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Only quit pygame display, not the entire pygame instance\n",
    "    if use_pygame:\n",
    "        pygame.display.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(learning_rate, discount_factor, explo_rate, explo_decay, min_explo_rate, num_episodes=10000, silent=True):  # Increased episodes\n",
    "    # Create environment\n",
    "    env = MultiAgentFrozenLake(map_name=\"4x4\", max_steps=100)\n",
    "    \n",
    "    # Create CentralQLearning agent with adjusted parameters\n",
    "    state_size = env.nrow * env.ncol * env.nrow * env.ncol\n",
    "    action_size = 4\n",
    "    agent = CentralQLearning(state_size=state_size, action_size=action_size, \n",
    "                           learning_rate=learning_rate,\n",
    "                           discount_factor=discount_factor,\n",
    "                           exploration_rate=explo_rate,\n",
    "                           exploration_decay=explo_decay,\n",
    "                           min_exploration_rate=min_explo_rate)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    episode_rewards = []\n",
    "    success_rate = []\n",
    "    success_window = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        # Run episode\n",
    "        while not done and not truncated:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        # Record episode success/failure\n",
    "        success = total_reward > 0\n",
    "        success_window.append(success)\n",
    "        if len(success_window) > 100:\n",
    "            success_window.pop(0)\n",
    "        \n",
    "        # Calculate success rate over last 100 episodes\n",
    "        current_success_rate = sum(success_window) / len(success_window)\n",
    "        success_rate.append(current_success_rate)\n",
    "        \n",
    "        # Record total reward\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if not silent and episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Success Rate: {current_success_rate:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(success_rate)\n",
    "    plt.title('Success Rate (100-episode moving average)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the agents...\")\n",
    "    \n",
    "    n_ep            = 5000\n",
    "    learning_rate   = 0.1\n",
    "    discount_factor = 0.99\n",
    "    explo_rate      = 1.0\n",
    "    explo_decay     = 0.999\n",
    "    min_explo_rate  = 0.05\n",
    "    trained_agent   = run_simulation(learning_rate, discount_factor, explo_rate, explo_decay, min_explo_rate, num_episodes=n_ep)\n",
    "    print(\"Training complete!\")\n",
    "    # Compare policies\n",
    "    print(\"Comparing trained policy with random policy...\")\n",
    "    # compare_policies(trained_agent)\n",
    "    \n",
    "    # Visualize the learned policy\n",
    "    print(\"Visualizing the learned policy...\")\n",
    "    visualize_policy(trained_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
