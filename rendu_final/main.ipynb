{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning\n",
    "In this notebook we create several reinforcement learning environments, based on *open AI*'s FrozenLake game:\n",
    "- a single-agent frozen lake environment\n",
    "- a multi-agent/ single goal environment\n",
    "- a multi-agent/ 4 goals environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import pygame\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import colorsys\n",
    "\n",
    "from utils import run_simulation, visualizePolicyCommonGoal\n",
    "from environments import MAPS, FrozenLakeOneGoal, createMap,FrozenLake4goals\n",
    "from algorithms import SingleGoalCentralQLearning, RandomPolicy, IndependentQLearning, AlternatingIQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi agents\n",
    "### Common goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the agents...\")\n",
    "    \n",
    "    num_agent = 2\n",
    "    \n",
    "    n_ep            = int(10e4)\n",
    "    learning_rate   = 0.1\n",
    "    discount_factor = 0.9\n",
    "    explo_rate      = 5.0\n",
    "    explo_decay     = 0.999\n",
    "    min_explo_rate  = 0.05\n",
    "    map_name        = '4x4'\n",
    "    \n",
    "    if map_name == None:\n",
    "        map_size = 4\n",
    "    else:\n",
    "        map_size = map_name[0]\n",
    "        \n",
    "    seed            = 0\n",
    "    \n",
    "    map_    = createMap(num_agent, map_size, seed=seed, map_name=None)\n",
    "    env     = FrozenLakeOneGoal(map_=map_, max_steps=100, num_agents=num_agent)\n",
    "    \n",
    "    trained_agent   = run_simulation(map_, num_agent, learning_rate, discount_factor, explo_rate, explo_decay, min_explo_rate, num_episodes=n_ep)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Visualize the learned policy\n",
    "    print(\"Visualizing the learned policy...\")\n",
    "    visualizePolicyCommonGoal(env, map_, trained_agent, num_episodes=4, num_agents=num_agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G H . . . . . G\n",
      ". . . H . . H H\n",
      ". H H H . H . .\n",
      ". H . A . . . A\n",
      ". . . A . . . H\n",
      ". H H A H . . .\n",
      "H H H H . . H .\n",
      "G . . . . H . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_agents=4\n",
    "env_params={\"num_agents\":n_agents, \n",
    "                \"grid_size\":(8, 8), \n",
    "                \"slip_prob\":0., \n",
    "                \"hole_prob\":0.3, \n",
    "                \"seed\":25, \n",
    "                \"collaboration_bonus\":0,\n",
    "                \"collision_penalty\":30}\n",
    "env=FrozenLake4goals(**env_params)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment there is 4 goals, the maximum reward is attained if all agents are evenly reparted:\n",
    "- we add a collision penalty of 30 everytime 2 agents are on the same tile\n",
    "- each goal is worth 100 at first and each time an agent reaches the reward the next reward is halved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iql_params = {\n",
    "            \"learning_rate\": 0.3,           # How quickly the agent incorporates new information (alpha)\n",
    "            \"discount_factor\": 0.99,        # How much future rewards are valued (gamma)\n",
    "            \"exploration_rate\": 1.0,        # Initial exploration rate (epsilon)\n",
    "            \"min_exploration_rate\": 0.05,   # Minimum exploration rate\n",
    "            \"exploration_decay\": 0.999,     # How quickly exploration decreases\n",
    "        }\n",
    "##train params\n",
    "max_episodes=10000   \n",
    "max_steps=200\n",
    "\n",
    "iql = IndependentQLearning(env,**iql_params)\n",
    "results = iql.train(episodes=max_episodes, max_steps=max_steps,verbose=True)\n",
    "print(\"\\nPolitiques ind√©pendantes apprises:\")\n",
    "iql.render_policy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
