{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning\n",
    "In this notebook we create several reinforcement learning environments, based on *open AI*'s FrozenLake game:\n",
    "- a single-agent frozen lake environment\n",
    "- a multi-agent/ single goal environment\n",
    "- a multi-agent/ 4 goals environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi agents\n",
    "### Common goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import IndependentQLearning,AlternatingIQL,QAgent,CentralizedQLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import FrozenLake4goals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G H . . . . . G\n",
      ". . . H . . H H\n",
      ". H H H . H . .\n",
      ". H . A . . . A\n",
      ". . . A . . . H\n",
      ". H H A H . . .\n",
      "H H H H . . H .\n",
      "G . . . . H . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_agents=4\n",
    "env_params={\"num_agents\":n_agents, \n",
    "                \"grid_size\":(8, 8), \n",
    "                \"slip_prob\":0., \n",
    "                \"hole_prob\":0.3, \n",
    "                \"seed\":25, \n",
    "                \"collaboration_bonus\":0,\n",
    "                \"collision_penalty\":30}\n",
    "env=FrozenLake4goals(**env_params)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment there is 4 goals, the maximum reward is attained if all agents are evenly reparted:\n",
    "- we add a collision penalty of 30 everytime 2 agents are on the same tile\n",
    "- each goal is worth 100 at first and each time an agent reaches the reward the next reward is halved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iql_params = {\n",
    "            \"learning_rate\": 0.3,           # How quickly the agent incorporates new information (alpha)\n",
    "            \"discount_factor\": 0.99,        # How much future rewards are valued (gamma)\n",
    "            \"exploration_rate\": 1.0,        # Initial exploration rate (epsilon)\n",
    "            \"min_exploration_rate\": 0.05,   # Minimum exploration rate\n",
    "            \"exploration_decay\": 0.999,     # How quickly exploration decreases\n",
    "        }\n",
    "##train params\n",
    "max_episodes=10000   \n",
    "max_steps=200\n",
    "\n",
    "iql = IndependentQLearning(env,**iql_params)\n",
    "results = iql.train(episodes=max_episodes, max_steps=max_steps,verbose=True)\n",
    "print(\"\\nPolitiques ind√©pendantes apprises:\")\n",
    "iql.render_policy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MARL_env)",
   "language": "python",
   "name": "marl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
