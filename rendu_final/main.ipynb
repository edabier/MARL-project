{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning\n",
    "In this notebook we create several reinforcement learning environments, based on *open AI*'s FrozenLake game:\n",
    "- a single-agent frozen lake environment\n",
    "- a multi-agent/ single goal environment\n",
    "- a multi-agent/ 4 goals environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import pygame\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import colorsys\n",
    "\n",
    "from environments import MAPS, FrozenLakeOneGoal, createMap,FrozenLake4goals\n",
    "from algorithms import SingleGoalCentralQLearning, RandomPolicy,IndependentQLearning,AlternatingIQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi agents\n",
    "### Common goal\n",
    "- **Running a learning algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agent, map_, num_agent, num_episodes=10000, silent=True):\n",
    "    # Create environment\n",
    "    env = FrozenLakeOneGoal(map_=map_, max_steps=100, num_agents=num_agent)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    episode_rewards = []\n",
    "    success_rate = []\n",
    "    success_window = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while not done and not truncated:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Record episode success/failure\n",
    "        success = total_reward > 0.5\n",
    "        success_window.append(success)\n",
    "        if len(success_window) > 200:\n",
    "            success_window.pop(0)\n",
    "        \n",
    "        # Calculate success rate over last 100 episodes\n",
    "        current_success_rate = sum(success_window) / len(success_window)\n",
    "        success_rate.append(current_success_rate)\n",
    "        \n",
    "        # Record total reward\n",
    "        # mean_reward = total_reward / step if step > 0 else 0\n",
    "        mean_reward = total_reward\n",
    "        episode_rewards.append(mean_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if not silent and episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {mean_reward}, Success Rate: {current_success_rate:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    window_size = 500\n",
    "    mean_rewards_smooth = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(mean_rewards_smooth)\n",
    "    plt.axhline(y=1, color='black', linestyle='--', linewidth=2)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(success_rate)\n",
    "    plt.title('Success Rate (500-episode moving average)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Visualizing the learned policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G H . . . . . G\n",
      ". . . H . . H H\n",
      ". H H H . H . .\n",
      ". H . A . . . A\n",
      ". . . A . . . H\n",
      ". H H A H . . .\n",
      "H H H H . . H .\n",
      "G . . . . H . G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_agents=4\n",
    "env_params={\"num_agents\":n_agents, \n",
    "                \"grid_size\":(8, 8), \n",
    "                \"slip_prob\":0., \n",
    "                \"hole_prob\":0.3, \n",
    "                \"seed\":25, \n",
    "                \"collaboration_bonus\":0,\n",
    "                \"collision_penalty\":30}\n",
    "env=FrozenLake4goals(**env_params)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment there is 4 goals, the maximum reward is attained if all agents are evenly reparted:\n",
    "- we add a collision penalty of 30 everytime 2 agents are on the same tile\n",
    "- each goal is worth 100 at first and each time an agent reaches the reward the next reward is halved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iql_params = {\n",
    "            \"learning_rate\": 0.3,           # How quickly the agent incorporates new information (alpha)\n",
    "            \"discount_factor\": 0.99,        # How much future rewards are valued (gamma)\n",
    "            \"exploration_rate\": 1.0,        # Initial exploration rate (epsilon)\n",
    "            \"min_exploration_rate\": 0.05,   # Minimum exploration rate\n",
    "            \"exploration_decay\": 0.999,     # How quickly exploration decreases\n",
    "        }\n",
    "##train params\n",
    "max_episodes=10000   \n",
    "max_steps=200\n",
    "\n",
    "iql = IndependentQLearning(env,**iql_params)\n",
    "results = iql.train(episodes=max_episodes, max_steps=max_steps,verbose=True)\n",
    "print(\"\\nPolitiques ind√©pendantes apprises:\")\n",
    "iql.render_policy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
