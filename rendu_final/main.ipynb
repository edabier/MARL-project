{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning\n",
    "In this notebook we create several reinforcement learning environments, based on *open AI*'s FrozenLake game:\n",
    "- a single-agent frozen lake environment\n",
    "- a multi-agent/ single goal environment\n",
    "- a multi-agent/ 4 goals environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import pygame\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import colorsys\n",
    "\n",
    "from environments import MAPS, FrozenLakeOneGoal, createMap\n",
    "from algorithms import SingleGoalCentralQLearning, RandomPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi agents\n",
    "### Common goal\n",
    "- **Running a learning algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agent, map_, num_agent, num_episodes=10000, silent=True):\n",
    "    # Create environment\n",
    "    env = FrozenLakeOneGoal(map_=map_, max_steps=100, num_agents=num_agent)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    episode_rewards = []\n",
    "    success_rate = []\n",
    "    success_window = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while not done and not truncated:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Record episode success/failure\n",
    "        success = total_reward > 0.5\n",
    "        success_window.append(success)\n",
    "        if len(success_window) > 200:\n",
    "            success_window.pop(0)\n",
    "        \n",
    "        # Calculate success rate over last 100 episodes\n",
    "        current_success_rate = sum(success_window) / len(success_window)\n",
    "        success_rate.append(current_success_rate)\n",
    "        \n",
    "        # Record total reward\n",
    "        # mean_reward = total_reward / step if step > 0 else 0\n",
    "        mean_reward = total_reward\n",
    "        episode_rewards.append(mean_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if not silent and episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {mean_reward}, Success Rate: {current_success_rate:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    window_size = 500\n",
    "    mean_rewards_smooth = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(mean_rewards_smooth)\n",
    "    plt.axhline(y=1, color='black', linestyle='--', linewidth=2)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(success_rate)\n",
    "    plt.title('Success Rate (500-episode moving average)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Visualizing the learned policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(map_, agent, num_episodes=2, max_steps=20, use_pygame=True, num_agents=2):\n",
    "    \"\"\"Visualize the learned policy\"\"\"\n",
    "    env = FrozenLakeOneGoal(map_=map_, num_agents=num_agents)\n",
    "    \n",
    "    # Action names for better visualization\n",
    "    action_names = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "    \n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            print(f\"\\n=== Test Episode {i+1} ===\")\n",
    "            if use_pygame:\n",
    "                env.render_pygame()\n",
    "            else:\n",
    "                print(\"Initial state:\")\n",
    "                env.render()\n",
    "            \n",
    "            while not done and not truncated and steps < max_steps:\n",
    "                # Use trained policy (no exploration)\n",
    "                state_tuple = tuple(state)\n",
    "                \n",
    "                # Get actions based on agent's Q-table\n",
    "                # This assumes agent.q_table is structured to handle num_agents\n",
    "                joint_actions = np.unravel_index(\n",
    "                    np.argmax(agent.q_table[state_tuple]),\n",
    "                    tuple([agent.action_size] * num_agents)\n",
    "                )\n",
    "                action = joint_actions\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Check for overlaps - this needs to be generalized for multiple agents\n",
    "                overlaps = []\n",
    "                for i in range(num_agents):\n",
    "                    for j in range(i+1, num_agents):\n",
    "                        # Compare positions of each pair of agents\n",
    "                        agent_i_pos = (next_state[i*2], next_state[i*2 + 1])\n",
    "                        agent_j_pos = (next_state[j*2], next_state[j*2 + 1])\n",
    "                        if agent_i_pos == agent_j_pos:\n",
    "                            overlaps.append((i, j))\n",
    "                \n",
    "                # Update state and reward\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Render with action information\n",
    "                print(f\"Step {steps}:\")\n",
    "                for agent_idx in range(num_agents):\n",
    "                    print(f\"Agent {agent_idx+1}: {action_names[action[agent_idx]]}\")\n",
    "                print(f\"Reward: {reward}\")\n",
    "                \n",
    "                if overlaps:\n",
    "                    print(\"Overlaps detected between agents:\", overlaps)\n",
    "                \n",
    "                if use_pygame:\n",
    "                    env.render_pygame()\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    env.render()\n",
    "                    time.sleep(0.5)\n",
    "            \n",
    "            print(f\"Episode finished after {steps} steps with total reward: {total_reward}\")\n",
    "            if done and total_reward > 0:\n",
    "                print(\"Success! At least one agent reached the goal.\")\n",
    "            elif done and total_reward <= 0:\n",
    "                print(\"Failed. Agents fell into holes or couldn't reach the goal.\")\n",
    "            else:\n",
    "                print(\"Truncated. Maximum steps reached.\")\n",
    "            \n",
    "            # Short pause between episodes\n",
    "            time.sleep(1)\n",
    "        \n",
    "    # Only close environment once after all episodes\n",
    "    finally:\n",
    "        # Make sure we close properly even if there's an exception\n",
    "        env.close()\n",
    "        if pygame.get_init():  # Check if pygame is still initialized\n",
    "            pygame.quit()  # Quit pygame completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Training the agents...\")\n",
    "    \n",
    "    num_agent = 2\n",
    "    n_ep            = 10000\n",
    "    learning_rate   = 0.1\n",
    "    discount_factor = 0.1\n",
    "    explo_rate      = 1.0\n",
    "    explo_decay     = 0.999\n",
    "    min_explo_rate  = 0.05\n",
    "    map_name        = None #'4x4'\n",
    "    map_size        = 4\n",
    "    \n",
    "    if map_name is None:\n",
    "        state_size  = (map_size * map_size) ** num_agent\n",
    "    else:\n",
    "        state_size  = (map_name[0] * map_name[0]) ** num_agent\n",
    "        \n",
    "    action_size     = 4\n",
    "    \n",
    "    seed            = 0\n",
    "    \n",
    "    map_ = createMap(num_agent, map_size, map_name, seed)\n",
    "    agent = SingleGoalCentralQLearning(state_size=state_size, action_size=action_size,num_agents=num_agent, \n",
    "                           learning_rate=learning_rate, discount_factor=discount_factor, exploration_rate=explo_rate,\n",
    "                           exploration_decay=explo_decay, min_exploration_rate=min_explo_rate)\n",
    "    \n",
    "    trained_agent   = run_simulation(agent, map_, num_agent, num_episodes=n_ep)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Visualize the learned policy\n",
    "    print(\"Visualizing the learned policy...\")\n",
    "    visualize_policy(map_, trained_agent, num_episodes=3, num_agents=num_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from environments import Frozen4goals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
