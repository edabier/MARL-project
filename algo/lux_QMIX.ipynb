{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LUX AI S3\n",
        "\n",
        "**Introduction**  \n",
        "Mars has been terraformed with help from over 600 space organizations, creating colonies thanks to growing lichen fields and a new atmosphere. Telescopes from Mars found mysterious ancient structures floating beyond our solar system, possibly from an old species. Ships are now exploring these relics to uncover their secrets. The Lux AI Challenge is a competition where you design bots to gather resources, optimize strategies, and outsmart opponents in a 1v1 game. Check the GitHub for code and join the Discord to connect with others.\n",
        "\n",
        "Here’s more detail on **Units & Actions** and **Winning** in the Lux AI Season 3 Challenge, explained simply:\n",
        "\n",
        "### Units & Actions\n",
        "In the game, your units are ships that you control on a 24x24 map. Each team gets a set number of ships (up to a max defined by the game rules). These ships start in one corner of the map, depending on your team, and they’re your tools to explore, gather resources, and fight the opponent.\n",
        "\n",
        "- **Energy**: Every ship starts with 100 energy and can go up to 400. Energy is like fuel—it powers everything your ship does. You gain energy from energy nodes on the map (these emit energy fields), but you can lose it from actions, nebula tiles (which sap energy), or enemy attacks. If a ship’s energy hits 0, it’s removed and might respawn later.\n",
        "\n",
        "- **Movement**: Ships can move one tile at a time in five directions: up, down, left, right, or stay still (center). Moving anywhere except staying still costs energy (called `unit_move_cost`, a random value between 1 and 5). You can’t move onto asteroids (they block you) or off the map (you just stay put and lose the energy). Friendly ships can stack on the same tile, which can be smart for teamwork but risky if the enemy attacks.\n",
        "\n",
        "- **Sap Actions**: This is your attack move. A ship can target a tile within a range (called `unit_sap_range`, random between 3 and 8) and sap energy from enemy ships there. It costs energy to use (called `unit_sap_cost`, random between 30 and 50). The targeted tile’s enemy ships lose that same amount of energy, and nearby enemy ships (on the 8 surrounding tiles) lose less (the cost times a drop-off factor, like 0.25, 0.5, or 1). If you miss, you waste energy, but hitting a stack of enemy ships can wipe them out fast.\n",
        "\n",
        "- **Collisions & Energy Void**: If enemy ships end a turn on the same tile, the team with the most total energy there wins—losers get removed. If it’s a tie, all ships on that tile are gone. Also, ships have an “energy void” field that saps energy from enemy ships on the four adjacent tiles (up, right, down, left). The strength depends on the ship’s energy and a random factor (0.0625 to 0.375). Stacking your ships can reduce this damage by splitting it among them.\n",
        "\n",
        "- **Vision**: Your ships determine what you see. Each has a sensor range (random, 2 to 4 tiles) that shows tiles around it. The farther a tile is, the weaker the vision, and nebula tiles can block it more (vision reduction of 0 to 3). If multiple ships’ vision overlaps, it gets stronger, helping you see through fog of war.\n",
        "\n",
        "### Winning\n",
        "The game is a best-of-5 match series, and each match lasts 100 time steps. Your goal is to beat the other team in more matches than they beat you.\n",
        "\n",
        "- **Match Win**: At the end of a match (after 100 steps), the team with the most relic points wins. Relic points come from relic nodes—special spots on the map where your ships earn points by sitting on hidden “point tiles” near them. These tiles are secret, so you have to guess and test to find them. Only one point per tile counts, even if you stack ships there.\n",
        "\n",
        "- **Tiebreakers**: If both teams have the same relic points, the winner is the team with more total unit energy across all their ships. If that’s tied too, the game picks a winner randomly.\n",
        "\n",
        "- **Game Win**: Out of the 5 matches, the team that wins the most is the overall winner. Since maps and random settings (like energy costs or sap range) stay the same across all 5 matches, you can learn the map and your opponent’s moves early on, then adjust to win the later matches.\n",
        "\n",
        "- **Turn Order**: Each step follows this order: move ships, do sap actions, resolve collisions and void fields, update energy (from map or nebulae), spawn new ships, check vision, move map objects (like asteroids), and count points. This happens 100 times per match, and what you do affects the next step.\n",
        "\n",
        "In short, your ships move, attack, and gather points while managing energy. To win, focus on finding relic points, outlasting your opponent’s energy, and adapting over the 5 matches!\n",
        "\n",
        "\n",
        "for more details: https://www.kaggle.com/competitions/lux-ai-season-3/overview "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tonSQMbWfFQg",
        "outputId": "ccd24f67-44bc-4ac8-81bd-50fca79a95f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  lux-ai-season-3.zip\n",
            "  inflating: README.md               \n",
            "  inflating: agent.py                \n",
            "  inflating: lux/__init__.py         \n",
            "  inflating: lux/kit.py              \n",
            "  inflating: lux/utils.py            \n",
            "  inflating: main.py                 \n"
          ]
        }
      ],
      "source": [
        "!unzip lux-ai-season-3.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VXiA_2YrRiy8",
        "outputId": "9b70242e-9911-4d2a-e6a0-f146407a85a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.11\n",
            "Collecting luxai-s3\n",
            "  Downloading luxai_s3-0.2.1-py3-none-any.whl.metadata (253 bytes)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from luxai-s3) (0.4.33)\n",
            "Collecting gymnax==0.0.8 (from luxai-s3)\n",
            "  Downloading gymnax-0.0.8-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tyro (from luxai-s3)\n",
            "  Downloading tyro-0.9.16-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (0.4.33)\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (0.1.89)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (0.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (6.0.2)\n",
            "Collecting gym>=0.26 (from gymnax==0.0.8->luxai-s3)\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from gymnax==0.0.8->luxai-s3) (0.13.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->luxai-s3) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from jax->luxai-s3) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->luxai-s3) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->luxai-s3) (1.13.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->luxai-s3) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->luxai-s3) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->luxai-s3)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->luxai-s3) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from tyro->luxai-s3) (4.12.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26->gymnax==0.0.8->luxai-s3) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26->gymnax==0.0.8->luxai-s3) (0.0.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->luxai-s3) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->luxai-s3) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex->gymnax==0.0.8->luxai-s3) (1.4.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex->gymnax==0.0.8->luxai-s3) (0.12.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax->gymnax==0.0.8->luxai-s3) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax->gymnax==0.0.8->luxai-s3) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax->gymnax==0.0.8->luxai-s3) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax->gymnax==0.0.8->luxai-s3) (0.1.72)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax==0.0.8->luxai-s3) (0.1.9)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax==0.0.8->luxai-s3) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->gymnax==0.0.8->luxai-s3) (2.2.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->luxai-s3) (0.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax==0.0.8->luxai-s3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax==0.0.8->luxai-s3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->gymnax==0.0.8->luxai-s3) (1.17.0)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax->flax->gymnax==0.0.8->luxai-s3) (1.12.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (4.25.6)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (4.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (3.21.0)\n",
            "Downloading luxai_s3-0.2.1-py3-none-any.whl (35 kB)\n",
            "Downloading gymnax-0.0.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.16-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827696 sha256=52e5909e18cb09d73c5931cab82aa2dd05177eba2b221d839a2d0738789741f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
            "Successfully built gym\n",
            "Installing collected packages: shtab, gym, tyro, gymnax, luxai-s3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2 gymnax-0.0.8 luxai-s3-0.2.1 shtab-1.7.1 tyro-0.9.16\n",
            "cp: cannot stat '../input/lux-ai-season-3/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install --upgrade luxai-s3\n",
        "!mkdir agent && cp -r ../input/lux-ai-season-3/* agent/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIqwfKYXsSGG",
        "outputId": "b277f465-05a7-4533-81e4-a14ac3690188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh0GRyJWKiZs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Helper function: returns a movement direction (0: stay, 1: up, 2: right, 3: down, 4: left)\n",
        "def direction_to(src, target):\n",
        "    ds = target - src\n",
        "    dx = ds[0]\n",
        "    dy = ds[1]\n",
        "    if dx == 0 and dy == 0:\n",
        "        return 0\n",
        "    if abs(dx) > abs(dy):\n",
        "        if dx > 0:\n",
        "            return 2\n",
        "        else:\n",
        "            return 4\n",
        "    else:\n",
        "        if dy > 0:\n",
        "            return 3\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "# ---------------------------------------\n",
        "# QMIX Network Components\n",
        "# ---------------------------------------\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_values = self.fc3(x)\n",
        "        return q_values\n",
        "\n",
        "class MixingNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, n_agents, state_dim):\n",
        "        super(MixingNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.state_dim = state_dim\n",
        "        self.n_agents = n_agents\n",
        "        self.hyper_w_1 = nn.Linear(state_dim, hidden_size * n_agents)\n",
        "        self.hyper_w_final = nn.Linear(state_dim, hidden_size)\n",
        "\n",
        "    def forward(self, q_values, states):\n",
        "        batch_size = q_values.size(0)\n",
        "        w1 = torch.abs(self.hyper_w_1(states)).view(batch_size, self.hidden_size, self.n_agents)\n",
        "        b1 = torch.zeros(batch_size, self.hidden_size).to(states.device)\n",
        "        hidden = torch.bmm(w1, q_values.unsqueeze(2)).squeeze(2) + b1\n",
        "        hidden = torch.relu(hidden)\n",
        "        w_final = torch.abs(self.hyper_w_final(states)).view(batch_size, 1, self.hidden_size)\n",
        "        b_final = torch.zeros(batch_size, 1).to(states.device)\n",
        "        q_tot = torch.bmm(w_final, hidden.unsqueeze(2)).squeeze(2) + b_final\n",
        "        return q_tot\n",
        "\n",
        "# ---------------------------------------\n",
        "# QMIX Agent Class\n",
        "# ---------------------------------------\n",
        "\n",
        "class QMIXAgent:\n",
        "    def __init__(self, player: str, env_cfg, training=False):\n",
        "        self.player = player\n",
        "        self.env_cfg = env_cfg\n",
        "        self.training = training\n",
        "        self.team_id = 0 if player == \"player_0\" else 1\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Define sizes\n",
        "        self.state_size = 27  # Matches the state vector size from get_state\n",
        "        self.action_size = 6  # 0: stay, 1-4: move directions, 5: sap\n",
        "        self.hidden_size = 256\n",
        "        self.n_agents = self.env_cfg[\"max_units\"]\n",
        "\n",
        "        # Initialize networks\n",
        "        self.q_network = QNetwork(self.state_size, self.hidden_size, self.action_size).to(self.device)\n",
        "        self.target_q_network = QNetwork(self.state_size, self.hidden_size, self.action_size).to(self.device)\n",
        "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.mixing_network = MixingNetwork(self.hidden_size, self.n_agents, self.state_size * self.n_agents).to(self.device)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(list(self.q_network.parameters()) + list(self.mixing_network.parameters()), lr=0.0005)\n",
        "\n",
        "        # Load model if not training\n",
        "        if not training:\n",
        "            self.load_model()\n",
        "\n",
        "        # Training variables\n",
        "        self.buffer = []\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.05\n",
        "        self.update_target_every = 1000\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # Visitation map for exploration bonus\n",
        "        self.visit_counts = np.zeros((env_cfg[\"map_width\"], env_cfg[\"map_height\"]))\n",
        "\n",
        "    def get_state(self, unit_id, obs, step):\n",
        "        unit_pos = obs[\"units\"][\"position\"][self.team_id][unit_id]\n",
        "        unit_energy = obs[\"units\"][\"energy\"][self.team_id][unit_id]\n",
        "\n",
        "        # Relic nodes (point tiles)\n",
        "        relic_nodes = np.array(obs[\"relic_nodes\"])\n",
        "        relic_mask = np.array(obs[\"relic_nodes_mask\"])\n",
        "        if not relic_mask.any():\n",
        "            closest_relic = np.array([-1, -1])\n",
        "        else:\n",
        "            visible_relics = relic_nodes[relic_mask]\n",
        "            distances = np.linalg.norm(visible_relics - unit_pos, axis=1)\n",
        "            closest_relic = visible_relics[np.argmin(distances)]\n",
        "\n",
        "        # Friendly units (up to 5)\n",
        "        friendly_positions = obs[\"units\"][\"position\"][self.team_id]\n",
        "        friendly_mask = obs[\"units_mask\"][self.team_id]\n",
        "        friendly_pos_list = [pos for i, pos in enumerate(friendly_positions) if friendly_mask[i] and i != unit_id]\n",
        "        friendly_pos_list = friendly_pos_list[:5] + [np.array([-1, -1])] * (5 - len(friendly_pos_list))\n",
        "\n",
        "        # Enemy units (up to 5)\n",
        "        opp_team_id = 1 - self.team_id\n",
        "        enemy_positions = obs[\"units\"][\"position\"][opp_team_id]\n",
        "        enemy_mask = obs[\"units_mask\"][opp_team_id]\n",
        "        enemy_pos_list = [pos for i, pos in enumerate(enemy_positions) if enemy_mask[i]]\n",
        "        enemy_pos_list = enemy_pos_list[:5] + [np.array([-1, -1])] * (5 - len(enemy_pos_list))\n",
        "\n",
        "        # Flatten positions\n",
        "        friendly_flat = np.concatenate(friendly_pos_list)\n",
        "        enemy_flat = np.concatenate(enemy_pos_list)\n",
        "\n",
        "        # On point tile flag\n",
        "        on_point_tile = int(any(np.array_equal(unit_pos, rn) for rn in relic_nodes[relic_mask]))\n",
        "\n",
        "        # Construct state vector\n",
        "        state = np.concatenate([\n",
        "            unit_pos, closest_relic, [unit_energy], [step / 505.0],\n",
        "            friendly_flat, enemy_flat, [on_point_tile]\n",
        "        ])\n",
        "        return torch.FloatTensor(state).to(self.device)\n",
        "\n",
        "    def get_valid_actions(self, unit_pos, unit_energy, obs):\n",
        "        valid_mask = [True] * 6  # 0: stay, 1-4: move, 5: sap\n",
        "        directions = [(0, 0), (0, -1), (1, 0), (0, 1), (-1, 0)]  # Corresponding to actions 0-4\n",
        "        tile_type = obs[\"map_features\"][\"tile_type\"]\n",
        "        map_width, map_height = tile_type.shape\n",
        "\n",
        "        # Validate movement actions\n",
        "        for i in range(1, 5):\n",
        "            next_pos = [unit_pos[0] + directions[i][0], unit_pos[1] + directions[i][1]]\n",
        "            if not (0 <= next_pos[0] < map_width and 0 <= next_pos[1] < map_height):\n",
        "                valid_mask[i] = False\n",
        "            elif tile_type[next_pos[0], next_pos[1]] == 2:  # Obstacle\n",
        "                valid_mask[i] = False\n",
        "            elif unit_energy < self.env_cfg[\"unit_move_cost\"]:\n",
        "                valid_mask[i] = False\n",
        "\n",
        "        # Validate sap action\n",
        "        if unit_energy < self.env_cfg[\"unit_sap_cost\"] * 2:  # Prevent sap if energy too low\n",
        "            valid_mask[5] = False\n",
        "\n",
        "        return valid_mask\n",
        "\n",
        "    def act(self, step, obs, remainingOverageTime=60):\n",
        "        unit_mask = np.array(obs[\"units_mask\"][self.team_id])\n",
        "        available_unit_ids = np.where(unit_mask)[0]\n",
        "        actions = np.zeros((self.env_cfg[\"max_units\"], 3), dtype=int)\n",
        "\n",
        "        for unit_id in available_unit_ids:\n",
        "            state = self.get_state(unit_id, obs, step)\n",
        "            unit_pos = obs[\"units\"][\"position\"][self.team_id][unit_id]\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state).cpu().numpy()\n",
        "            valid_mask = self.get_valid_actions(unit_pos, obs[\"units\"][\"energy\"][self.team_id][unit_id], obs)\n",
        "\n",
        "            # Check if unit is on a point tile\n",
        "            on_point_tile = state[-1].item() == 1\n",
        "\n",
        "            if on_point_tile:\n",
        "                q_values[0] += 10.0  # Large positive bias for staying\n",
        "\n",
        "            # Penalize moving to tiles occupied by friendly units\n",
        "            friendly_positions = [pos for i, pos in enumerate(obs[\"units\"][\"position\"][self.team_id]) if obs[\"units_mask\"][self.team_id][i] and i != unit_id]\n",
        "            directions = [(0, 0), (0, -1), (1, 0), (0, 1), (-1, 0)]\n",
        "            for i in range(1, 5):  # Movement actions\n",
        "                next_pos = [unit_pos[0] + directions[i][0], unit_pos[1] + directions[i][1]]\n",
        "                if any(np.array_equal(next_pos, pos) for pos in friendly_positions):\n",
        "                    q_values[i] -= 5.0  # Penalty for moving to friendly-occupied tile\n",
        "\n",
        "            # Boltzmann exploration\n",
        "            temperature = max(0.1, self.epsilon)\n",
        "            q_values_valid = q_values.copy()\n",
        "            q_values_valid[~np.array(valid_mask)] = -float('inf')\n",
        "            probs = torch.softmax(torch.tensor(q_values_valid) / temperature, dim=0).numpy()\n",
        "            action = np.random.choice(len(valid_mask), p=probs / probs.sum())  # Normalize probs\n",
        "\n",
        "            if action == 5:  # Sap action\n",
        "                opp_team_id = 1 - self.team_id\n",
        "                opp_positions = np.array(obs[\"units\"][\"position\"][opp_team_id])\n",
        "                opp_energies = np.array(obs[\"units\"][\"energy\"][opp_team_id])\n",
        "                opp_mask = np.array(obs[\"units_mask\"][opp_team_id])\n",
        "                valid_targets = [(pos, energy) for i, (pos, energy) in enumerate(zip(opp_positions, opp_energies)) if opp_mask[i] and pos[0] != -1]\n",
        "                if valid_targets:\n",
        "                    # Score targets: prioritize enemies on point tiles or with high energy\n",
        "                    relic_nodes = np.array(obs[\"relic_nodes\"])\n",
        "                    relic_mask = np.array(obs[\"relic_nodes_mask\"])\n",
        "                    point_tiles = [tuple(rn) for rn in relic_nodes[relic_mask]]\n",
        "                    scores = [energy + 10 if tuple(pos) in point_tiles else energy for pos, energy in valid_targets]\n",
        "                    target_pos = valid_targets[np.argmax(scores)][0]\n",
        "                    actions[unit_id] = [5, target_pos[0], target_pos[1]]\n",
        "                else:\n",
        "                    actions[unit_id] = [0, 0, 0]\n",
        "            else:  # Movement action\n",
        "                actions[unit_id] = [action, 0, 0]\n",
        "\n",
        "            # Update visitation counts during training\n",
        "            if self.training:\n",
        "                pos = unit_pos.astype(int)\n",
        "                self.visit_counts[pos[0], pos[1]] += 1\n",
        "\n",
        "        if self.training:\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        return actions\n",
        "\n",
        "    def learn(self, batch):\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = torch.stack([torch.stack(s) for s in states]).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device).unsqueeze(-1)\n",
        "        next_states = torch.stack([torch.stack(s) for s in next_states]).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device).unsqueeze(-1)\n",
        "\n",
        "        # Add exploration bonus and reward for staying on point tiles\n",
        "        for i, state_batch in enumerate(states):\n",
        "            for unit_state in state_batch:\n",
        "                pos = unit_state[:2].cpu().numpy().astype(int)\n",
        "                exploration_bonus = 0.01 / (1 + self.visit_counts[pos[0], pos[1]])\n",
        "                rewards[i] += exploration_bonus\n",
        "                # Reward for staying on point tiles\n",
        "                if unit_state[-1].item() == 1:\n",
        "                    rewards[i] += 1.0  # Strong bonus for staying\n",
        "\n",
        "        global_state = states.view(states.size(0), -1)\n",
        "        next_global_state = next_states.view(next_states.size(0), -1)\n",
        "\n",
        "        q_values = self.q_network(states)\n",
        "        q_values_selected = q_values.gather(2, actions.unsqueeze(-1)).squeeze(-1)\n",
        "        q_tot = self.mixing_network(q_values_selected, global_state)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_q_network(next_states).max(dim=2)[0]\n",
        "            next_q_tot = self.mixing_network(next_q_values, next_global_state)\n",
        "            targets = rewards + (1 - dones) * 0.99 * next_q_tot\n",
        "\n",
        "        loss = nn.MSELoss()(q_tot, targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.step_counter += 1\n",
        "        if self.step_counter % self.update_target_every == 0:\n",
        "            self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
        "            logging.info(f\"Target network updated at step {self.step_counter}\")\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save({\n",
        "            'q_network_state': self.q_network.state_dict(),\n",
        "            'mixing_network_state': self.mixing_network.state_dict(),\n",
        "            'optimizer_state': self.optimizer.state_dict()\n",
        "        }, f'qmix_model_{self.player}.pth')\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            model_file_name = f\"qmix_model_{self.player}.pth\"\n",
        "            model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), model_file_name)\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "            self.q_network.load_state_dict(checkpoint['q_network_state'])\n",
        "            self.mixing_network.load_state_dict(checkpoint['mixing_network_state'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "            logging.info(f\"Loaded model for {self.player} from {model_path}\")\n",
        "        except FileNotFoundError:\n",
        "            logging.warning(f\"No trained model found for {self.player} at {model_path}. Starting from scratch.\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Training and Evaluation Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "def train(player_0, player_1, num_games=100, save_interval=10):\n",
        "    env = LuxAIS3GymEnv(numpy_output=True)\n",
        "    obs, info = env.reset(seed=42)\n",
        "    env_cfg = info[\"params\"]\n",
        "\n",
        "    agents = {\"player_0\": player_0, \"player_1\": player_1}\n",
        "\n",
        "    logging.info(\"Starting training...\")\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for game in range(num_games):\n",
        "        obs, info = env.reset()\n",
        "        game_done = False\n",
        "        step = 0\n",
        "        total_reward_0 = 0\n",
        "        total_reward_1 = 0\n",
        "\n",
        "        while not game_done:\n",
        "            actions = {}\n",
        "            for player in [\"player_0\", \"player_1\"]:\n",
        "                actions[player] = agents[player].act(step, obs[player])\n",
        "\n",
        "            next_obs, rewards, terminated, truncated, info = env.step(actions)\n",
        "            dones = {k: terminated[k] or truncated[k] for k in terminated}\n",
        "\n",
        "            total_reward_0 += rewards[\"player_0\"]\n",
        "            total_reward_1 += rewards[\"player_1\"]\n",
        "\n",
        "            for player in [\"player_0\", \"player_1\"]:\n",
        "                if agents[player].training:\n",
        "                    agent = agents[player]\n",
        "                    states_all = [agent.get_state(unit_id, obs[player], step) for unit_id in range(agent.n_agents)]\n",
        "                    next_states_all = [agent.get_state(unit_id, next_obs[player], step + 1) for unit_id in range(agent.n_agents)]\n",
        "                    actions_all = [actions[player][unit_id][0] for unit_id in range(agent.n_agents)]\n",
        "                    reward = rewards[player].item()\n",
        "                    done = dones[player].item()\n",
        "                    agent.buffer.append((states_all, actions_all, reward, next_states_all, done))\n",
        "\n",
        "                    if len(agent.buffer) >= 128:\n",
        "                        batch = random.sample(agent.buffer, 128)\n",
        "                        agent.learn(batch)\n",
        "\n",
        "            if any(dones.values()):\n",
        "                game_done = True\n",
        "            step += 1\n",
        "            obs = next_obs\n",
        "\n",
        "        print(f\"Game {game + 1}/{num_games} finished. Total rewards: player_0={total_reward_0}, player_1={total_reward_1}\")\n",
        "        logging.info(f\"Game {game + 1}/{num_games} finished with rewards: player_0={total_reward_0}, player_1={total_reward_1}\")\n",
        "\n",
        "        if (game + 1) % save_interval == 0:\n",
        "            for player in [\"player_0\", \"player_1\"]:\n",
        "                agents[player].save_model()\n",
        "            logging.info(f\"Models saved after {game + 1} games.\")\n",
        "            print(f\"Models saved after {game + 1} games.\")\n",
        "\n",
        "    env.close()\n",
        "    logging.info(\"Training finished.\")\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "def evaluate(player_0, player_1, num_games=10):\n",
        "    env = LuxAIS3GymEnv(numpy_output=True)\n",
        "    obs, info = env.reset(seed=42)\n",
        "    env_cfg = info[\"params\"]\n",
        "\n",
        "    agents = {\"player_0\": player_0, \"player_1\": player_1}\n",
        "    total_rewards = {\"player_0\": 0, \"player_1\": 0}\n",
        "\n",
        "    logging.info(\"Starting evaluation...\")\n",
        "    print(\"Evaluation started...\")\n",
        "\n",
        "    for game in range(num_games):\n",
        "        obs, info = env.reset()\n",
        "        game_done = False\n",
        "        game_reward_0 = 0\n",
        "        game_reward_1 = 0\n",
        "\n",
        "        while not game_done:\n",
        "            actions = {}\n",
        "            for player in [\"player_0\", \"player_1\"]:\n",
        "                actions[player] = agents[player].act(0, obs[player])\n",
        "            obs, rewards, terminated, truncated, info = env.step(actions)\n",
        "            dones = {k: terminated[k] or truncated[k] for k in terminated}\n",
        "\n",
        "            game_reward_0 += rewards[\"player_0\"]\n",
        "            game_reward_1 += rewards[\"player_1\"]\n",
        "\n",
        "            if any(dones.values()):\n",
        "                game_done = True\n",
        "\n",
        "        total_rewards[\"player_0\"] += game_reward_0\n",
        "        total_rewards[\"player_1\"] += game_reward_1\n",
        "\n",
        "        print(f\"Evaluation Game {game + 1}/{num_games} finished. Rewards: player_0={game_reward_0}, player_1={game_reward_1}\")\n",
        "        logging.info(f\"Game {game + 1}/{num_games} finished with rewards: player_0={game_reward_0}, player_1={game_reward_1}\")\n",
        "\n",
        "    print(f\"Evaluation finished. Total rewards: player_0={total_rewards['player_0']}, player_1={total_rewards['player_1']}\")\n",
        "    logging.info(f\"Total evaluation rewards: {total_rewards}\")\n",
        "\n",
        "    env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Gw7dl6JEOf3f"
      },
      "outputs": [],
      "source": [
        "# Initialize environment\n",
        "env = LuxAIS3GymEnv(numpy_output=True)\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Initialize agents\n",
        "player_0 = QMIXAgent(\"player_0\", info[\"params\"], training=True)\n",
        "player_1 = QMIXAgent(\"player_1\", info[\"params\"], training=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0lqweLgKqAy",
        "outputId": "f2033557-fca8-4aba-e440-37f2bd8c26d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "Game 1/10 finished. Total rewards: player_0=306, player_1=709\n",
            "Game 2/10 finished. Total rewards: player_0=610, player_1=405\n",
            "Game 3/10 finished. Total rewards: player_0=812, player_1=203\n",
            "Game 4/10 finished. Total rewards: player_0=812, player_1=203\n",
            "Game 5/10 finished. Total rewards: player_0=610, player_1=405\n",
            "Game 6/10 finished. Total rewards: player_0=305, player_1=710\n",
            "Game 7/10 finished. Total rewards: player_0=1015, player_1=0\n",
            "Game 8/10 finished. Total rewards: player_0=1015, player_1=0\n",
            "Game 9/10 finished. Total rewards: player_0=306, player_1=709\n",
            "Game 10/10 finished. Total rewards: player_0=508, player_1=507\n",
            "Models saved after 10 games.\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "# Train the agents\n",
        "train(player_0, player_1, num_games=10, save_interval=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrAtN5RQyYzl",
        "outputId": "26439f45-cd3b-45b5-c81d-9109e6d8463d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "Game 1/10 finished. Total rewards: player_0=306, player_1=709\n",
            "Game 2/10 finished. Total rewards: player_0=103, player_1=912\n",
            "Game 3/10 finished. Total rewards: player_0=304, player_1=711\n",
            "Game 4/10 finished. Total rewards: player_0=507, player_1=508\n",
            "Game 5/10 finished. Total rewards: player_0=507, player_1=508\n",
            "Game 6/10 finished. Total rewards: player_0=305, player_1=710\n",
            "Game 7/10 finished. Total rewards: player_0=102, player_1=913\n",
            "Game 8/10 finished. Total rewards: player_0=407, player_1=608\n",
            "Game 9/10 finished. Total rewards: player_0=1, player_1=1014\n",
            "Game 10/10 finished. Total rewards: player_0=305, player_1=710\n",
            "Models saved after 10 games.\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "train(player_1, player_0, num_games=10, save_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5RsD7oe8IRs"
      },
      "outputs": [],
      "source": [
        "# Train the agents\n",
        "train(player_0, player_1, num_games=10, save_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgLTp2318J46"
      },
      "outputs": [],
      "source": [
        "train(player_1, player_0, num_games=10, save_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6AGs57KrnH",
        "outputId": "43b4eb71-daa4-41c3-f8ea-c9f74ca5762a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation started...\n",
            "Evaluation Game 1/10 finished. Rewards: player_0=1014, player_1=1\n",
            "Evaluation Game 2/10 finished. Rewards: player_0=406, player_1=609\n",
            "Evaluation Game 3/10 finished. Rewards: player_0=709, player_1=306\n",
            "Evaluation Game 4/10 finished. Rewards: player_0=711, player_1=304\n",
            "Evaluation Game 5/10 finished. Rewards: player_0=711, player_1=304\n",
            "Evaluation Game 6/10 finished. Rewards: player_0=1015, player_1=0\n",
            "Evaluation Game 7/10 finished. Rewards: player_0=305, player_1=710\n",
            "Evaluation Game 8/10 finished. Rewards: player_0=1015, player_1=0\n",
            "Evaluation Game 9/10 finished. Rewards: player_0=204, player_1=811\n",
            "Evaluation Game 10/10 finished. Rewards: player_0=1014, player_1=1\n",
            "Evaluation finished. Total rewards: player_0=7104, player_1=3046\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate the trained agents\n",
        "evaluate(player_0, player_1, num_games=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBUqajuVk1Ou",
        "outputId": "6c5b9ad0-048f-45e5-85ce-2149c1f07c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time Elapsed:  27.874014854431152\n",
            "Rewards:  {'player_0': array(1, dtype=int32), 'player_1': array(4, dtype=int32)}\n"
          ]
        }
      ],
      "source": [
        "!luxai-s3 main.py main.py --output=replay.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSNENoicjOS1"
      },
      "outputs": [],
      "source": [
        "!tar -czvf submission.tar.gz agent.py qmix_model_player_0.pth qmix_model_player_1.pth main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PAMMCQYfngC",
        "outputId": "b8b06e4f-46eb-4cc3-ef06-fdee3594d2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent.py\n",
            "qmix_model_player_0.pth\n",
            "main.py\n",
            "qmix_model_player_1.pth\n",
            "lux/\n",
            "lux/utils.py\n",
            "lux/__pycache__/\n",
            "lux/__pycache__/kit.cpython-311.pyc\n",
            "lux/__pycache__/utils.cpython-311.pyc\n",
            "lux/__pycache__/__init__.cpython-311.pyc\n",
            "lux/__init__.py\n",
            "lux/kit.py\n"
          ]
        }
      ],
      "source": [
        "!tar -czvf LAGx20-flip-2-7.tar.gz agent.py qmix_model_player_0.pth main.py qmix_model_player_1.pth lux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaKJIlZcj13r"
      },
      "outputs": [],
      "source": [
        "# next goals : enhance the reward system\n",
        "# print the match scores while training and evaluating"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
